{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TextCNN模型\n",
    "1. setting arg\n",
    "2. model\n",
    "3. load data\n",
    "4. train model\n",
    "\n",
    "\n",
    "思考的逻辑是model（包括训练和预测），model需要什么参数，记下来，放置在arg，（是上到下吗）\n",
    "在写程序的时候，应该要设置好arg，model，训练，出结果\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本包的载入\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import operator\n",
    "import random\n",
    "\n",
    "# 基本数据处理包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# 特定的数据清洗包\n",
    "import wordninja\n",
    "from geopy.geocoders import Bing\n",
    "import emoji\n",
    "\n",
    "# torch, torchtext\n",
    "from torch.nn import init\n",
    "import torchtext.data as data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(1992)\n",
    "\n",
    "def argsSetting():\n",
    "    parser = argparse.ArgumentParser(description='TextCNN text classifier')\n",
    "    # learning\n",
    "    parser.add_argument('-lr', type=float, default=0.001, help='initial learning rate [default: 0.001]')\n",
    "    parser.add_argument('-epochs', type=int, default=256, help='number of epochs for train [default: 256]')\n",
    "    parser.add_argument('-batch-size', type=int, default=128, help='batch size for training [default: 128]')\n",
    "    parser.add_argument('-log-interval', type=int, default=1,\n",
    "                        help='how many steps to wait before logging training status [default: 1]')\n",
    "    parser.add_argument('-test-interval', type=int, default=100,\n",
    "                        help='how many steps to wait before testing [default: 100]')\n",
    "    parser.add_argument('-save-dir', type=str, default='snapshot', help='where to save the snapshot')\n",
    "    parser.add_argument('-early-stopping', type=int, default=1000,\n",
    "                        help='iteration numbers to stop without performance increasing')\n",
    "    parser.add_argument('-save-best', type=bool, default=True, help='whether to save when get best performance')\n",
    "    \n",
    "    # loading data\n",
    "    parser.add_argument('-dataset-path', type=str, default= str(os.getcwd()), \n",
    "                    help='where the dataset locates [default: os.getcwd()]')\n",
    "    parser.add_argument('-dataset-train', type=str, default= 'dataset_train.csv', \n",
    "                    help='the document of training set [default: dataset_train.csv]')\n",
    "    parser.add_argument('-dataset-val', type=str, default= 'dataset_val.csv', \n",
    "                    help='the document of validation set [default: dataset_val.csv]')\n",
    "    \n",
    "    # model\n",
    "    parser.add_argument('-dropout', type=float, default=0.5, help='the probability for dropout [default: 0.5]')\n",
    "    parser.add_argument('-max-norm', type=float, default=3.0, help='l2 constraint of parameters [default: 3.0]')\n",
    "    parser.add_argument('-embedding-dim', type=int, default=128, help='number of embedding dimension [default: 128]')\n",
    "    parser.add_argument('-sentence-max-length', type=int, default= 128, help='max length of sentence as input [default: 128]')\n",
    "    parser.add_argument('-min-freq', type=int, default= 5, help='minimal frequency allowed on vocabulary [default: 5]')\n",
    "    parser.add_argument('-filter-num', type=int, default=100, help='number of each size of filter')\n",
    "    parser.add_argument('-filter-sizes', type=str, default='3,4,5',\n",
    "                        help='comma-separated filter sizes to use for convolution')\n",
    "    parser.add_argument('-padding-sizes', type=str, default='1,2,2',\n",
    "                        help='padding sizes to use for convolution')\n",
    "\n",
    "    parser.add_argument('-static', type=bool, default=False, help='whether to use static pre-trained word vectors')\n",
    "    parser.add_argument('-non-static', type=bool, default=False, help='whether to fine-tune static pre-trained word vectors')\n",
    "    parser.add_argument('-multichannel', type=bool, default=False, help='whether to use 2 channel of word vectors')\n",
    "    parser.add_argument('-pretrained-name', type=str, default='sgns.zhihu.word',\n",
    "                        help='filename of pre-trained word vectors')\n",
    "    parser.add_argument('-pretrained-path', type=str, default='pretrained', help='path of pre-trained word vectors')\n",
    "\n",
    "    # device\n",
    "    parser.add_argument('-device', type=int, default=-1, help='device to use for iterate data, -1 mean cpu [default: -1]')\n",
    "\n",
    "    # option\n",
    "    parser.add_argument('-snapshot', type=str, default=None, help='filename of model snapshot [default: None]')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args\n",
    "    \n",
    "# build args\n",
    "args = argsSetting()\n",
    "\n",
    "# fix args\n",
    "args.dataset_path = 'D:/比赛/disaster/nlp-getting-started/'\n",
    "args.dataset_train = 'dataset_train.csv'\n",
    "args.dataset_val = 'dataset_val.csv'\n",
    "\n",
    "args.min_freq = 5\n",
    "args.batch_size = 64\n",
    "args.sentence_max_length = 40\n",
    "args.test_interval = 50\n",
    "args.early_stopping = 500\n",
    "args.pretrained_name = 'glove.840B.300d.txt'\n",
    "args.pretrained_path = 'C:/Users/123/glove.840B.300d.txt/'\n",
    "args.static = True\n",
    "\n",
    "args.save_dir = 'D://比赛//disaster//nlp-getting-started//model_save//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        class_num = args.class_num\n",
    "        chanel_num = 1\n",
    "        filter_num = args.filter_num\n",
    "        filter_sizes = args.filter_sizes\n",
    "        padding_sizes = args.padding_sizes\n",
    "\n",
    "        vocabulary_size = args.vocabulary_size\n",
    "        embedding_dimension = args.embedding_dim\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension)\n",
    "        if args.static:\n",
    "            self.embedding = self.embedding.from_pretrained(args.vectors, freeze=not args.non_static)\n",
    "        if args.multichannel:\n",
    "            self.embedding2 = nn.Embedding(vocabulary_size, embedding_dimension).from_pretrained(args.vectors)\n",
    "            chanel_num += 1\n",
    "        else:\n",
    "            self.embedding2 = None\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(chanel_num, filter_num, (size, embedding_dimension), padding = (padding, 0)) \n",
    "             for size, padding in zip(filter_sizes, padding_sizes)])\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * filter_num, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.embedding2:\n",
    "            x = torch.stack([self.embedding(x), self.embedding2(x)], dim=1)\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(item, item.size(2)).squeeze(2) for item in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(train_iter, val_iter, model, args):\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    steps = 0\n",
    "    best_acc = 0\n",
    "    last_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for batch in train_iter:\n",
    "            feature, target = batch.text, batch.target\n",
    "            if args.cuda:\n",
    "                feature, target = feature.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(feature.data.permute(1,0))\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "            if steps % args.log_interval == 0:\n",
    "                corrects = (torch.max(logits, 1)[1].view(target.size()).data == target.data).sum()\n",
    "                train_acc = 100.0 * corrects / batch.batch_size\n",
    "                sys.stdout.write(\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps,\n",
    "                                                                             loss.item(),\n",
    "                                                                             train_acc,\n",
    "                                                                             corrects,\n",
    "                                                                             batch.batch_size))\n",
    "            if steps % args.test_interval == 0:\n",
    "                val_acc = eval(val_iter, model, args)\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    last_step = steps\n",
    "                    if args.save_best:\n",
    "                        print('Saving best model, acc: {:.4f}%\\n'.format(best_acc))\n",
    "                        save(model, args.save_dir, 'best', steps)\n",
    "                else:\n",
    "                    if steps - last_step >= args.early_stopping:\n",
    "                        print('\\nearly stop by {} steps, acc: {:.4f}%'.format(args.early_stopping, best_acc))\n",
    "                        raise KeyboardInterrupt\n",
    "\n",
    "def eval(data_iter, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for batch in data_iter:\n",
    "        feature, target = batch.text, batch.target\n",
    "        if args.cuda:\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "        logits = model(feature.data.permute(1,0))\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logits, 1)\n",
    "                     [1].view(target.size()).data == target.data).sum()\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * corrects / size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss,\n",
    "                                                                       accuracy,\n",
    "                                                                       corrects,\n",
    "                                                                       size))\n",
    "    return accuracy\n",
    "\n",
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "\tBATCH_SIZE=64\n",
      "\tCLASS_NUM=3\n",
      "\tCUDA=False\n",
      "\tDATASET_PATH=D:/比赛/disaster/nlp-getting-started/\n",
      "\tDATASET_TRAIN=dataset_train.csv\n",
      "\tDATASET_VAL=dataset_val.csv\n",
      "\tDEVICE=-1\n",
      "\tDROPOUT=0.5\n",
      "\tEARLY_STOPPING=500\n",
      "\tEMBEDDING_DIM=300\n",
      "\tEPOCHS=256\n",
      "\tFILTER_NUM=100\n",
      "\tFILTER_SIZES=[3, 4, 5]\n",
      "\tLOG_INTERVAL=1\n",
      "\tLR=0.001\n",
      "\tMAX_NORM=3.0\n",
      "\tMIN_FREQ=5\n",
      "\tMULTICHANNEL=False\n",
      "\tNON_STATIC=False\n",
      "\tPADDING_SIZES=[1, 2, 2]\n",
      "\tPRETRAINED_NAME=glove.840B.300d.txt\n",
      "\tPRETRAINED_PATH=C:/Users/123/glove.840B.300d.txt/\n",
      "\tSAVE_BEST=True\n",
      "\tSAVE_DIR=D://比赛//disaster//nlp-getting-started//model_save//\n",
      "\tSENTENCE_MAX_LENGTH=40\n",
      "\tSNAPSHOT=None\n",
      "\tSTATIC=True\n",
      "\tTEST_INTERVAL=50\n",
      "\tVOCABULARY_SIZE=2851\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "function to load data\n",
    "\"\"\"\n",
    "def load_word_vectors(model_name, model_path):\n",
    "    vectors = Vectors(name=model_name, cache=model_path)\n",
    "    return vectors\n",
    "\n",
    "def load_dataset(text_field, label_field, args, **kwargs):\n",
    "    # load train.csv and val.csv\n",
    "    train, val = TabularDataset.splits(\n",
    "        path= args.dataset_path, # the root directory where the data lies\n",
    "        train= args.dataset_train, validation= args.dataset_val,\n",
    "        format='csv',\n",
    "        skip_header=True,\n",
    "        fields=[(\"text\", text_field), (\"target\", label_field)])\n",
    "    # load vocabulary and embedding\n",
    "    if args.static and args.pretrained_name and args.pretrained_path:\n",
    "        vectors = load_word_vectors(args.pretrained_name, args.pretrained_path)\n",
    "        text_field.build_vocab(train, val, vectors=vectors, min_freq = args.min_freq)\n",
    "    else:\n",
    "        text_field.build_vocab(train, val, min_freq = args.min_freq)\n",
    "    label_field.build_vocab(train, val)\n",
    "    # produce iterator\n",
    "    train_iter, val_iter = BucketIterator.splits(\n",
    "                        (train, val),\n",
    "                         batch_sizes=(args.batch_size, args.sentence_max_length),\n",
    "                         sort_key=lambda x: len(x.text), # field sorted by len\n",
    "                         **kwargs)\n",
    "    return train_iter, val_iter\n",
    "\n",
    "print('Loading data...')\n",
    "tokenize = lambda x: x.split()\n",
    "y_tokenize = lambda y: int(y)\n",
    "text_field = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "label_field = Field(sequential=False, tokenize = y_tokenize, use_vocab=False)\n",
    "train_iter, val_iter = load_dataset(text_field, label_field, args, device=-1, repeat=False, shuffle=True)\n",
    "\n",
    "\n",
    "args.vocabulary_size = len(text_field.vocab)\n",
    "if args.static:\n",
    "    args.embedding_dim = text_field.vocab.vectors.size()[-1]\n",
    "    args.vectors = text_field.vocab.vectors\n",
    "if args.multichannel:\n",
    "    args.static = True\n",
    "    args.non_static = True\n",
    "args.class_num = len(label_field.vocab)\n",
    "args.cuda = args.device != -1 and torch.cuda.is_available()\n",
    "args.filter_sizes = [int(size) for size in args.filter_sizes.split(',')]\n",
    "args.padding_sizes = [int(padding) for padding in args.padding_sizes.split(',')]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters:')\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "    if attr in {'vectors'}:\n",
    "        continue\n",
    "    print('\\t{}={}'.format(attr.upper(), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[50] - loss: 0.464242  acc: 75.0000%(48/64)\n",
      "Evaluation - loss: 0.011517  acc: 79.1721%(1205/1522) \n",
      "\n",
      "Saving best model, acc: 79.1721%\n",
      "\n",
      "Batch[100] - loss: 0.421368  acc: 76.5625%(49/64)\n",
      "Evaluation - loss: 0.011213  acc: 80.1577%(1220/1522) \n",
      "\n",
      "Saving best model, acc: 80.1577%\n",
      "\n",
      "Batch[150] - loss: 0.417345  acc: 81.2500%(52/64)\n",
      "Evaluation - loss: 0.011494  acc: 77.8581%(1185/1522) \n",
      "\n",
      "Batch[200] - loss: 0.312370  acc: 85.9375%(55/64)\n",
      "Evaluation - loss: 0.010746  acc: 80.2891%(1222/1522) \n",
      "\n",
      "Saving best model, acc: 80.2891%\n",
      "\n",
      "Batch[250] - loss: 0.367827  acc: 79.6875%(51/64)\n",
      "Evaluation - loss: 0.011330  acc: 80.6176%(1227/1522) \n",
      "\n",
      "Saving best model, acc: 80.6176%\n",
      "\n",
      "Batch[300] - loss: 0.232155  acc: 92.1875%(59/64)\n",
      "Evaluation - loss: 0.010999  acc: 80.8804%(1231/1522) \n",
      "\n",
      "Saving best model, acc: 80.8804%\n",
      "\n",
      "Batch[350] - loss: 0.159873  acc: 95.3125%(61/64)\n",
      "Evaluation - loss: 0.011562  acc: 80.8147%(1230/1522) \n",
      "\n",
      "Batch[400] - loss: 0.230375  acc: 90.6250%(58/64))\n",
      "Evaluation - loss: 0.012137  acc: 78.9093%(1201/1522) \n",
      "\n",
      "Batch[450] - loss: 0.352024  acc: 87.5000%(56/64)\n",
      "Evaluation - loss: 0.012230  acc: 78.5808%(1196/1522) \n",
      "\n",
      "Batch[500] - loss: 0.095030  acc: 98.4375%(63/64))\n",
      "Evaluation - loss: 0.013248  acc: 78.1209%(1189/1522) \n",
      "\n",
      "Batch[550] - loss: 0.090846  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.013463  acc: 78.8436%(1200/1522) \n",
      "\n",
      "Batch[600] - loss: 0.119126  acc: 96.8750%(62/64))\n",
      "Evaluation - loss: 0.013462  acc: 78.3837%(1193/1522) \n",
      "\n",
      "Batch[650] - loss: 0.134727  acc: 98.4375%(63/64))\n",
      "Evaluation - loss: 0.013463  acc: 79.4350%(1209/1522) \n",
      "\n",
      "Batch[700] - loss: 0.061197  acc: 98.4375%(63/64))\n",
      "Evaluation - loss: 0.014100  acc: 79.4350%(1209/1522) \n",
      "\n",
      "Batch[750] - loss: 0.176581  acc: 95.3125%(61/64))\n",
      "Evaluation - loss: 0.014244  acc: 79.1721%(1205/1522) \n",
      "\n",
      "Batch[800] - loss: 0.036525  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.014897  acc: 78.0552%(1188/1522) \n",
      "\n",
      "\n",
      "early stop by 500 steps, acc: 80.8804%\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "text_cnn = TextCNN(args)\n",
    "if args.snapshot:\n",
    "    print('\\nLoading model from {}...\\n'.format(args.snapshot))\n",
    "    text_cnn.load_state_dict(torch.load(args.snapshot))\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(args.device)\n",
    "    text_cnn = text_cnn.cuda()\n",
    "try:\n",
    "    train(train_iter, val_iter, text_cnn, args)\n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting from training early')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
